{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=78>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Science'>}\n",
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=16>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Maths'>}\n",
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=89>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Biology'>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame({'Department1':[78,16,89],\n",
    "                   'Department2': ['Science','Maths','Biology']})\n",
    " \n",
    "df\n",
    "new_df_val = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "new_df_val \n",
    "for i in new_df_val .take(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=178>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Chemistry'>}\n",
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=965>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Maths'>}\n",
      "{'Department1': <tf.Tensor: shape=(), dtype=int64, numpy=156>, 'Department2': <tf.Tensor: shape=(), dtype=string, numpy=b'Biology'>}\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Department1':[178,965,156],\n",
    "                   'Department2': ['Chemistry','Maths','Biology']})\n",
    " \n",
    "df\n",
    "new_df_val = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "new_df_val \n",
    "for i in new_df_val .take(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as rq\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa # this is needed for the parquet file\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive, fixed, VBox\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the Dublin bus gz files \n",
    "def load_Files(direc, files, comtype):\n",
    "    columns = ['Timestamp', 'LineID', 'Direction', 'JourneyPatternID', 'TimeFrame', 'VehicleJourneyID', 'Operator', 'Congestion', 'LonWGS84', 'LatWGS84', 'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f, compression=comtype, delimiter=',', header=0, names=columns, parse_dates=True, low_memory=True)\n",
    "\n",
    "def crackit_open(busFile):\n",
    "    import zipfile as zip\n",
    "    # Zip creates its own folders - no need to check for folder existence\n",
    "    with zip.ZipFile(busFile,  mode='r') as arc: \n",
    "        arc.extractall('./Data/Bus/Gz/')  \n",
    "    files = os.listdir('./Data/Bus/Gz/')\n",
    "    DBfiles = [f for f in files if f.endswith('.gz')]\n",
    "    df = pd.concat(load_Files('./Data/Bus/Gz/', DBfiles, 'gzip'), copy = False)\n",
    "    return df\n",
    "\n",
    "def shapiro_test(x):\n",
    "    p_val = stats.shapiro(x)[1]\n",
    "    status = 'passed'\n",
    "    color = 'blue'\n",
    "    if p_val < 0.05:\n",
    "        status = 'failed'\n",
    "        color = 'red'\n",
    "    return status, color, p_val\n",
    "\n",
    "def custom_scatterplot(df1, col1=''):\n",
    "    df1 = df1[df1[\"LineID\"]==col1]\n",
    "    f = plt.figure()\n",
    "    f, ax = plt.subplots(figsize=(11.5, 11.5))\n",
    "    ax = f.add_subplot(projection='3d')\n",
    "    ax.scatter(df1['LonWGS84'], df1['LatWGS84'], df1['Hour'], alpha=0.6, color=df1['Colour'])\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Hour')\n",
    "    dcol = str(col1)\n",
    "    plt.savefig('./Images/Img_' + dcol + '_Longitude_Latitude_Hour.svg')\n",
    "    df1.to_parquet('./Data/Bus/LineID_' + dcol + '.parquet')\n",
    "    \n",
    "    \n",
    "def custom_barplot(df1, col1=''):\n",
    "    if len(df1[col1]) > 5000: # added this to the function because of warnings about the size of data being used with shapiro test\n",
    "            sampleSize = 5000\n",
    "    else:\n",
    "        sampleSize = len(df1[col1])\n",
    "    df1 = df1.sample(sampleSize) #shapiro test is unreliable over 5000 https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test and performance reasons\n",
    "    f, ax = plt.subplots(2,2, figsize=(11.5, 11.5))\n",
    "    ax = ax.reshape(-1)\n",
    "    df1[col1].plot(ax=ax[0], kind='hist')\n",
    "    ax[0].set_title('Histogram of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[1], kind='kde')\n",
    "    ax[1].set_title('Density Plot of {}'.format(col1))\n",
    "    ax3 = plt.subplot(223)\n",
    "    stats.probplot(df[col1], plot=plt)\n",
    "    ax[2].set_title('QQ Plot of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[3], kind='box')\n",
    "    ax[3].set_title('Box Plot of {}'.format(col1))\n",
    "    status, color, p_val = shapiro_test(df1[col1]) \n",
    "    f.suptitle('Normality test for {} {} (p_value = {})'.format(col1, status, p_val), color=color, fontsize=12)\n",
    "\n",
    "def num_missing(x):\n",
    "    return len(x.index)-x.count()\n",
    "\n",
    "def num_unique(x):\n",
    "    return len(np.unique(x))\n",
    "\n",
    "def load_csv_Files(direc, files):\n",
    "\n",
    "    for f in files:\n",
    "        # need to get number of rows to skip \n",
    "        temp=pd.read_csv(direc + f,sep='^',header=None,prefix='X')\n",
    "        temp2=temp.X0.str.split(',',expand=True)\n",
    "        del temp['X0']\n",
    "        temp=pd.concat([temp,temp2],axis=1)\n",
    "        cols = list(range(0,temp.shape[1]))\n",
    "\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f,  delimiter=',', header=0,  parse_dates=True, low_memory=True, skiprows=14, usecols=cols, na_values='NAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder exists\n",
      "Bus Data folder exists\n",
      "Weather data folder exists\n",
      "Weather folder exisits\n",
      "Images folder exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./Data'):\n",
    "    print('Data folder exists')\n",
    "\n",
    "if os.path.exists('./Data/Bus'):\n",
    "    print('Bus Data folder exists')\n",
    "else:\n",
    "    os.makedirs('./Data/Bus')\n",
    "\n",
    "if os.path.exists('./Data/MetEirrean/'):\n",
    "    print('Weather data folder exists')\n",
    "else:\n",
    "    os.makedirs('./Data/MetEirrean/')\n",
    "\n",
    "if os.path.exists('./Zips/MetEirrean'):\n",
    "    print('Weather folder exisits')\n",
    "else:\n",
    "    os.makedirs('./Zips/MetEirrean/')\n",
    "\n",
    "if os.path.exists('./Images/'):\n",
    "    print('Images folder exists')\n",
    "else:\n",
    "    os.makedirs('./Images/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file exists, we have already downloaded the Dublin Bus Zip data, crack it open\n",
      "siri.20130101.csv.gz\n",
      "siri.20130102.csv.gz\n",
      "siri.20130103.csv.gz\n",
      "siri.20130104.csv.gz\n",
      "siri.20130105.csv.gz\n",
      "siri.20130106.csv.gz\n",
      "siri.20130107.csv.gz\n",
      "siri.20130108.csv.gz\n",
      "siri.20130109.csv.gz\n",
      "siri.20130110.csv.gz\n",
      "siri.20130111.csv.gz\n",
      "siri.20130112.csv.gz\n",
      "siri.20130113.csv.gz\n",
      "siri.20130114.csv.gz\n",
      "siri.20130115.csv.gz\n",
      "siri.20130116.csv.gz\n",
      "siri.20130117.csv.gz\n",
      "siri.20130118.csv.gz\n",
      "siri.20130119.csv.gz\n",
      "siri.20130120.csv.gz\n",
      "siri.20130121.csv.gz\n",
      "siri.20130122.csv.gz\n",
      "siri.20130123.csv.gz\n",
      "siri.20130124.csv.gz\n",
      "siri.20130125.csv.gz\n",
      "siri.20130126.csv.gz\n",
      "siri.20130127.csv.gz\n",
      "siri.20130128.csv.gz\n",
      "siri.20130129.csv.gz\n",
      "siri.20130130.csv.gz\n",
      "siri.20130131.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Check parquet file existence before downloading - iof starting from afresh this takes a long time\n",
    "if os.path.exists('./Data/CleanedBusData.parquet'):\n",
    "    print(\"Parquest file exists, means the Bus data has been downloaded already \")\n",
    "    df = pd.read_parquet('./Data/CleanedBusData.parquet')\n",
    "    if os.path.exists('./Data/WeatherandBusData.parquet'):\n",
    "        print('Weather and bus data combined exists')\n",
    "        mdf = pd.read_parquet('./Data/WeatherandBusData.parquet')\n",
    "elif os.path.exists('./Zips/Bus/DublinBusdata.zip'):\n",
    "    print(\"Zip file exists, we have already downloaded the Dublin Bus Zip data, crack it open\")\n",
    "    df = crackit_open('./Zips/Bus/DublinBusdata.zip')\n",
    "else:\n",
    "    os.makedirs('./Zips/Bus/', exist_ok = True)\n",
    "    url = \"https://opendata.dublincity.ie/TrafficOpenData/sir010113-310113.zip\"\n",
    "    busFile = rq.urlretrieve(url, './Zips/Bus/DublinBusdata.zip' )  \n",
    "    df = crackit_open('./Zips/Bus/DublinBusdata.zip')\n",
    "\n",
    "df = df.sample(20000000)\n",
    "### Read the Bus data in to a Pandas dataframe - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>1.358318e+15</td>\n",
       "      <td>7.383705e+11</td>\n",
       "      <td>1.356998e+15</td>\n",
       "      <td>1.357680e+15</td>\n",
       "      <td>1.358328e+15</td>\n",
       "      <td>1.358955e+15</td>\n",
       "      <td>1.359633e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LineID</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>7.841271e+01</td>\n",
       "      <td>1.194481e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>8.300000e+01</td>\n",
       "      <td>7.470000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleJourneyID</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>1.027393e+04</td>\n",
       "      <td>6.682941e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.744000e+03</td>\n",
       "      <td>4.814000e+03</td>\n",
       "      <td>6.861000e+03</td>\n",
       "      <td>9.998560e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Congestion</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>1.366826e-02</td>\n",
       "      <td>1.161096e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LonWGS84</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>-6.270799e+00</td>\n",
       "      <td>8.182937e-02</td>\n",
       "      <td>-6.617517e+00</td>\n",
       "      <td>-6.306787e+00</td>\n",
       "      <td>-6.261670e+00</td>\n",
       "      <td>-6.232140e+00</td>\n",
       "      <td>-6.053033e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LatWGS84</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>5.334426e+01</td>\n",
       "      <td>5.437022e-02</td>\n",
       "      <td>5.306802e+01</td>\n",
       "      <td>5.331951e+01</td>\n",
       "      <td>5.334611e+01</td>\n",
       "      <td>5.337347e+01</td>\n",
       "      <td>5.360873e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delay</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>-5.337298e+01</td>\n",
       "      <td>4.848823e+02</td>\n",
       "      <td>-1.504500e+04</td>\n",
       "      <td>-2.660000e+02</td>\n",
       "      <td>-2.100000e+01</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>1.161220e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlockID</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>1.027233e+05</td>\n",
       "      <td>1.823115e+05</td>\n",
       "      <td>3.900000e+02</td>\n",
       "      <td>1.601400e+04</td>\n",
       "      <td>4.010800e+04</td>\n",
       "      <td>8.301200e+04</td>\n",
       "      <td>8.350020e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleID</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>3.548041e+04</td>\n",
       "      <td>3.307243e+03</td>\n",
       "      <td>2.804700e+04</td>\n",
       "      <td>3.331500e+04</td>\n",
       "      <td>3.352900e+04</td>\n",
       "      <td>3.803000e+04</td>\n",
       "      <td>4.307800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StopID</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>2.647753e+03</td>\n",
       "      <td>2.126668e+03</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>8.100000e+02</td>\n",
       "      <td>2.039000e+03</td>\n",
       "      <td>4.320000e+03</td>\n",
       "      <td>7.552000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AtStop</th>\n",
       "      <td>17007287.0</td>\n",
       "      <td>2.725230e-01</td>\n",
       "      <td>4.452575e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count          mean           std           min  \\\n",
       "Timestamp         17007287.0  1.358318e+15  7.383705e+11  1.356998e+15   \n",
       "LineID            17007287.0  7.841271e+01  1.194481e+02  1.000000e+00   \n",
       "VehicleJourneyID  17007287.0  1.027393e+04  6.682941e+04  1.000000e+00   \n",
       "Congestion        17007287.0  1.366826e-02  1.161096e-01  0.000000e+00   \n",
       "LonWGS84          17007287.0 -6.270799e+00  8.182937e-02 -6.617517e+00   \n",
       "LatWGS84          17007287.0  5.334426e+01  5.437022e-02  5.306802e+01   \n",
       "Delay             17007287.0 -5.337298e+01  4.848823e+02 -1.504500e+04   \n",
       "BlockID           17007287.0  1.027233e+05  1.823115e+05  3.900000e+02   \n",
       "VehicleID         17007287.0  3.548041e+04  3.307243e+03  2.804700e+04   \n",
       "StopID            17007287.0  2.647753e+03  2.126668e+03  2.000000e+00   \n",
       "AtStop            17007287.0  2.725230e-01  4.452575e-01  0.000000e+00   \n",
       "\n",
       "                           25%           50%           75%           max  \n",
       "Timestamp         1.357680e+15  1.358328e+15  1.358955e+15  1.359633e+15  \n",
       "LineID            1.800000e+01  4.000000e+01  8.300000e+01  7.470000e+02  \n",
       "VehicleJourneyID  2.744000e+03  4.814000e+03  6.861000e+03  9.998560e+05  \n",
       "Congestion        0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  \n",
       "LonWGS84         -6.306787e+00 -6.261670e+00 -6.232140e+00 -6.053033e+00  \n",
       "LatWGS84          5.331951e+01  5.334611e+01  5.337347e+01  5.360873e+01  \n",
       "Delay            -2.660000e+02 -2.100000e+01  1.140000e+02  1.161220e+05  \n",
       "BlockID           1.601400e+04  4.010800e+04  8.301200e+04  8.350020e+05  \n",
       "VehicleID         3.331500e+04  3.352900e+04  3.803000e+04  4.307800e+04  \n",
       "StopID            8.100000e+02  2.039000e+03  4.320000e+03  7.552000e+03  \n",
       "AtStop            0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LineID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JourneyPatternID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleJourneyID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Operator</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Congestion</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LonWGS84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LatWGS84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delay</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlockID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StopID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AtStop</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  missing\n",
       "Timestamp               0\n",
       "LineID                  0\n",
       "JourneyPatternID        0\n",
       "VehicleJourneyID        0\n",
       "Operator                0\n",
       "Congestion              0\n",
       "LonWGS84                0\n",
       "LatWGS84                0\n",
       "Delay                   0\n",
       "BlockID                 0\n",
       "VehicleID               0\n",
       "StopID                  0\n",
       "AtStop                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'Direction' in df.columns:\n",
    "    df = df.drop(['Direction'], axis='columns')\n",
    "if 'TimeFrame' in df.columns:\n",
    "    df = df.drop(['TimeFrame'], axis='columns')\n",
    "\"\"\" if {'VehicleJourneyID', 'JourneyPatternID'}.issubset(df.columns):  \n",
    "    df['VehicleJourneyID'] = df['JourneyPatternID'] + '_' + df['VehicleJourneyID'].astype('str')\n",
    "    df = df.drop(['JourneyPatternID','VehicleJourneyID'], axis='columns') \"\"\"\n",
    "\n",
    "df = df.dropna() \n",
    "df = df.drop_duplicates()\n",
    "\n",
    "temp_df = df.describe().T\n",
    "missing_df = pd.DataFrame(df.apply(num_missing, axis=0)) \n",
    "missing_df.columns = ['missing']  # type: ignore\n",
    "\n",
    "display(temp_df)\n",
    "\n",
    "display(missing_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 17007287 Rows and 13 columns\n",
      "The types of columns are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp             int64\n",
       "LineID              float64\n",
       "JourneyPatternID     object\n",
       "VehicleJourneyID      int64\n",
       "Operator             object\n",
       "Congestion            int64\n",
       "LonWGS84            float64\n",
       "LatWGS84            float64\n",
       "Delay                 int64\n",
       "BlockID               int64\n",
       "VehicleID             int64\n",
       "StopID              float64\n",
       "AtStop                int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print ('The data has {} Rows and {} columns'.format(df.shape[0],df.shape[1]))\n",
    "print(\"The types of columns are:\")\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:103\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:487\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    484\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    485\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    488\u001b[0m     element,\n\u001b[0;32m    489\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for 265870     5289\n636318     3376\n387747     5524\n1595855    5431\n107694     6479\n           ... \n572278     1600\n61405      5074\n309707     4976\n627806      513\n1142444     929\nName: VehicleJourneyID, Length: 17007287, dtype: int64 with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_df_val \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices(\u001b[39mdict\u001b[39;49m(df))\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   4707\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4708\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m   4709\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4710\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:108\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 108\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mn:\\GitHubSource\\pytorch-tutorials\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "new_df_val = tf.data.Dataset.from_tensor_slices(dict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e7a37b4b1135b43340b5e417bac95578913bb71e4f0a908af7470400087c85c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
